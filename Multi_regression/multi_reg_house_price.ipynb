{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import copy,math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"/workspaces/87071081/machine_learning/multiple_linear_regression/data/house_price_train.csv\")\n",
    "test=pd.read_csv(\"/workspaces/87071081/machine_learning/multiple_linear_regression/data/house_price_test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>...</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>3</td>\n",
       "      <td>8450</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>12.247694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>9600</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>12.109011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>3</td>\n",
       "      <td>11250</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>12.317167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>9550</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>272</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>11.849398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>3</td>\n",
       "      <td>14260</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>12.429216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>14115</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>320</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>700</td>\n",
       "      <td>10</td>\n",
       "      <td>2009</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>11.870600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>10084</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2007</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>12.634603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>60</td>\n",
       "      <td>3</td>\n",
       "      <td>10382</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>228</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>350</td>\n",
       "      <td>11</td>\n",
       "      <td>2009</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>12.206073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>6120</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>205</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2008</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>11.774520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>190</td>\n",
       "      <td>3</td>\n",
       "      <td>7420</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2008</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>11.678440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 72 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Id  MSSubClass  MSZoning  LotArea  Street  LotShape  \\\n",
       "0           0   1          60         3     8450       1         3   \n",
       "1           1   2          20         3     9600       1         3   \n",
       "2           2   3          60         3    11250       1         0   \n",
       "3           3   4          70         3     9550       1         0   \n",
       "4           4   5          60         3    14260       1         0   \n",
       "5           5   6          50         3    14115       1         0   \n",
       "6           6   7          20         3    10084       1         3   \n",
       "7           7   8          60         3    10382       1         0   \n",
       "8           8   9          50         4     6120       1         3   \n",
       "9           9  10         190         3     7420       1         3   \n",
       "\n",
       "   LandContour  Utilities  LotConfig  ...  EnclosedPorch  3SsnPorch  \\\n",
       "0            3          0          4  ...              0          0   \n",
       "1            3          0          2  ...              0          0   \n",
       "2            3          0          4  ...              0          0   \n",
       "3            3          0          0  ...            272          0   \n",
       "4            3          0          2  ...              0          0   \n",
       "5            3          0          4  ...              0        320   \n",
       "6            3          0          4  ...              0          0   \n",
       "7            3          0          0  ...            228          0   \n",
       "8            3          0          4  ...            205          0   \n",
       "9            3          0          0  ...              0          0   \n",
       "\n",
       "   ScreenPorch  PoolArea  MiscVal  MoSold  YrSold  SaleType  SaleCondition  \\\n",
       "0            0         0        0       2    2008         8              4   \n",
       "1            0         0        0       5    2007         8              4   \n",
       "2            0         0        0       9    2008         8              4   \n",
       "3            0         0        0       2    2006         8              0   \n",
       "4            0         0        0      12    2008         8              4   \n",
       "5            0         0      700      10    2009         8              4   \n",
       "6            0         0        0       8    2007         8              4   \n",
       "7            0         0      350      11    2009         8              4   \n",
       "8            0         0        0       4    2008         8              0   \n",
       "9            0         0        0       1    2008         8              4   \n",
       "\n",
       "   SalePrice  \n",
       "0  12.247694  \n",
       "1  12.109011  \n",
       "2  12.317167  \n",
       "3  11.849398  \n",
       "4  12.429216  \n",
       "5  11.870600  \n",
       "6  12.634603  \n",
       "7  12.206073  \n",
       "8  11.774520  \n",
       "9  11.678440  \n",
       "\n",
       "[10 rows x 72 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data set has two unwanted features, (Unnamed and Id) so, we will\n",
    "remove those. At the end you can see we have a sales price.\n",
    "sales price is our prediction price, our Y. And all other features are our X.\n",
    "I have attached a file below which describes our features in detail. Go\n",
    "have a look on it. house-features\n",
    "Also the original dataset had a lot of categorical data, which had\n",
    "character or string in their datafield. Now we cannot train our machine\n",
    "learning model with categorical (or string) data fields. So I have already\n",
    "pre-processed the data, converting categorical features into numerical\n",
    "features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping unwanted data from dataset\n",
    "train = train.drop([\"Unnamed: 0\", \"Id\"], axis = 1)\n",
    "test = test.drop([\"Unnamed: 0\", \"Id\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seperating target and training values\n",
    "train_data = train.values\n",
    "Y = train_data[:, -1].reshape(train_data.shape[0], 1)\n",
    "X = train_data[:, :-1]\n",
    "\n",
    "test_data = test.values\n",
    "Y_test = test_data[:, -1].reshape(test_data.shape[0], 1)\n",
    "X_test = test_data[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train : (1200, 69)\n",
      "Shape of Y_train : (1200, 1)\n",
      "Shape of X_test : (258, 69)\n",
      "Shape of Y_test : (258, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X_train :\", X.shape)\n",
    "print(\"Shape of Y_train :\", Y.shape)\n",
    "print(\"Shape of X_test :\", X_test.shape)\n",
    "print(\"Shape of Y_test :\", Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=np.array(X)\n",
    "y_train=np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Shape: (1200, 69), X Type:<class 'numpy.ndarray'>)\n",
      "[[6.0000e+01 3.0000e+00 8.4500e+03 ... 2.0080e+03 8.0000e+00 4.0000e+00]\n",
      " [2.0000e+01 3.0000e+00 9.6000e+03 ... 2.0070e+03 8.0000e+00 4.0000e+00]\n",
      " [6.0000e+01 3.0000e+00 1.1250e+04 ... 2.0080e+03 8.0000e+00 4.0000e+00]\n",
      " ...\n",
      " [2.0000e+01 3.0000e+00 9.1000e+03 ... 2.0090e+03 8.0000e+00 4.0000e+00]\n",
      " [2.0000e+01 3.0000e+00 1.1235e+04 ... 2.0060e+03 8.0000e+00 4.0000e+00]\n",
      " [2.0000e+01 3.0000e+00 9.3530e+03 ... 2.0060e+03 7.0000e+00 0.0000e+00]]\n",
      "y Shape: (1200, 1), y Type:<class 'numpy.ndarray'>)\n",
      "[[12.24769432]\n",
      " [12.10901093]\n",
      " [12.31716669]\n",
      " ...\n",
      " [12.08953883]\n",
      " [11.90496755]\n",
      " [11.66177641]]\n"
     ]
    }
   ],
   "source": [
    "# data is stored in numpy array/matrix\n",
    "print(f\"X Shape: {X_train.shape}, X Type:{type(X_train)})\")\n",
    "print(X_train)\n",
    "print(f\"y Shape: {y_train.shape}, y Type:{type(y_train)})\")\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_init shape: (69,), b_init type: <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "values_array = np.random.uniform(0, 100, size=69)\n",
    "\n",
    "b_init = 785.1811367994083\n",
    "w_init = np.array(values_array)\n",
    "print(f\"w_init shape: {w_init.shape}, b_init type: {type(b_init)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_loop(x,w,b):\n",
    "    n=x.shape[0]\n",
    "    p=0\n",
    "    for i in range(n):\n",
    "        p_i=x[i]*w[i]\n",
    "        p=p+p_i\n",
    "    p=p+b\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_vec shape (69,), x_vec value: [6.000e+01 3.000e+00 8.450e+03 1.000e+00 3.000e+00 3.000e+00 0.000e+00\n",
      " 4.000e+00 0.000e+00 5.000e+00 2.000e+00 2.000e+00 0.000e+00 5.000e+00\n",
      " 7.000e+00 5.000e+00 2.003e+03 2.003e+03 1.000e+00 0.000e+00 1.200e+01\n",
      " 1.300e+01 1.000e+00 1.960e+02 2.000e+00 4.000e+00 2.000e+00 2.000e+00\n",
      " 3.000e+00 3.000e+00 2.000e+00 7.060e+02 5.000e+00 0.000e+00 1.500e+02\n",
      " 8.560e+02 1.000e+00 0.000e+00 1.000e+00 4.000e+00 8.540e+02 0.000e+00\n",
      " 1.710e+03 1.000e+00 0.000e+00 2.000e+00 1.000e+00 3.000e+00 1.000e+00\n",
      " 2.000e+00 6.000e+00 0.000e+00 1.000e+00 1.000e+00 2.000e+00 4.000e+00\n",
      " 4.000e+00 2.000e+00 0.000e+00 6.100e+01 0.000e+00 0.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00 2.000e+00 2.008e+03 8.000e+00 4.000e+00]\n",
      "f_wb shape (), prediction: 1268807.4776452782\n"
     ]
    }
   ],
   "source": [
    "# get a row from our training data\n",
    "x_vec = X_train[0,:]\n",
    "print(f\"x_vec shape {x_vec.shape}, x_vec value: {x_vec}\")\n",
    "\n",
    "# make a prediction\n",
    "f_wb = predict_single_loop(x_vec, w_init, b_init)\n",
    "print(f\"f_wb shape {f_wb.shape}, prediction: {f_wb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_vec shape (69,), x_vec value: [6.000e+01 3.000e+00 8.450e+03 1.000e+00 3.000e+00 3.000e+00 0.000e+00\n",
      " 4.000e+00 0.000e+00 5.000e+00 2.000e+00 2.000e+00 0.000e+00 5.000e+00\n",
      " 7.000e+00 5.000e+00 2.003e+03 2.003e+03 1.000e+00 0.000e+00 1.200e+01\n",
      " 1.300e+01 1.000e+00 1.960e+02 2.000e+00 4.000e+00 2.000e+00 2.000e+00\n",
      " 3.000e+00 3.000e+00 2.000e+00 7.060e+02 5.000e+00 0.000e+00 1.500e+02\n",
      " 8.560e+02 1.000e+00 0.000e+00 1.000e+00 4.000e+00 8.540e+02 0.000e+00\n",
      " 1.710e+03 1.000e+00 0.000e+00 2.000e+00 1.000e+00 3.000e+00 1.000e+00\n",
      " 2.000e+00 6.000e+00 0.000e+00 1.000e+00 1.000e+00 2.000e+00 4.000e+00\n",
      " 4.000e+00 2.000e+00 0.000e+00 6.100e+01 0.000e+00 0.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00 2.000e+00 2.008e+03 8.000e+00 4.000e+00]\n",
      "f_wb shape (), prediction: 1268807.4776452782\n"
     ]
    }
   ],
   "source": [
    "# get a row from our training data\n",
    "x_vec = X_train[0,:]\n",
    "print(f\"x_vec shape {x_vec.shape}, x_vec value: {x_vec}\")\n",
    "\n",
    "# make a prediction\n",
    "f_wb = predict_single_loop(x_vec, w_init, b_init)\n",
    "print(f\"f_wb shape {f_wb.shape}, prediction: {f_wb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, w, b):\n",
    "    \"\"\"\n",
    "    single predict using linear regression\n",
    "    Args:\n",
    "      x (ndarray): Shape (n,) example with multiple features\n",
    "      w (ndarray): Shape (n,) model parameters\n",
    "      b (scalar):             model parameter\n",
    "\n",
    "    Returns:\n",
    "      p (scalar):  prediction\n",
    "    \"\"\"\n",
    "    p = np.dot(x, w) + b\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_vec shape (69,), x_vec value: [6.000e+01 3.000e+00 8.450e+03 1.000e+00 3.000e+00 3.000e+00 0.000e+00\n",
      " 4.000e+00 0.000e+00 5.000e+00 2.000e+00 2.000e+00 0.000e+00 5.000e+00\n",
      " 7.000e+00 5.000e+00 2.003e+03 2.003e+03 1.000e+00 0.000e+00 1.200e+01\n",
      " 1.300e+01 1.000e+00 1.960e+02 2.000e+00 4.000e+00 2.000e+00 2.000e+00\n",
      " 3.000e+00 3.000e+00 2.000e+00 7.060e+02 5.000e+00 0.000e+00 1.500e+02\n",
      " 8.560e+02 1.000e+00 0.000e+00 1.000e+00 4.000e+00 8.540e+02 0.000e+00\n",
      " 1.710e+03 1.000e+00 0.000e+00 2.000e+00 1.000e+00 3.000e+00 1.000e+00\n",
      " 2.000e+00 6.000e+00 0.000e+00 1.000e+00 1.000e+00 2.000e+00 4.000e+00\n",
      " 4.000e+00 2.000e+00 0.000e+00 6.100e+01 0.000e+00 0.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00 2.000e+00 2.008e+03 8.000e+00 4.000e+00]\n",
      "f_wb shape (), prediction: 1268807.4776452787\n"
     ]
    }
   ],
   "source": [
    "# get a row from our training data\n",
    "x_vec = X_train[0,:]\n",
    "print(f\"x_vec shape {x_vec.shape}, x_vec value: {x_vec}\")\n",
    "\n",
    "# make a prediction\n",
    "f_wb = predict(x_vec,w_init, b_init)\n",
    "print(f\"f_wb shape {f_wb.shape}, prediction: {f_wb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, w, b):\n",
    "    \"\"\"\n",
    "    compute cost\n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters\n",
    "      b (scalar)       : model parameter\n",
    "\n",
    "    Returns:\n",
    "      cost (scalar): cost\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    cost = 0.0\n",
    "    for i in range(m):\n",
    "        f_wb_i = np.dot(X[i], w) + b           #(n,)(n,) = scalar (see np.dot)\n",
    "        cost = cost + (f_wb_i - y[i])**2       #scalar\n",
    "    cost = cost / (2 * m)                      #scalar\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at optimal w : [1.26698119e+12]\n"
     ]
    }
   ],
   "source": [
    "# Compute and display cost using our pre-chosen optimal parameters.\n",
    "cost = compute_cost(X_train, y_train, w_init, b_init)\n",
    "print(f'Cost at optimal w : {cost}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression\n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters\n",
    "      b (scalar)       : model parameter\n",
    "\n",
    "    Returns:\n",
    "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w.\n",
    "      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b.\n",
    "    \"\"\"\n",
    "    m,n = X.shape           #(number of examples, number of features)\n",
    "    dj_dw = np.zeros((n,))\n",
    "    dj_db = 0.\n",
    "\n",
    "    for i in range(m):\n",
    "        err = (np.dot(X[i], w) + b) - y[i]\n",
    "        for j in range(n):\n",
    "            dj_dw[j] = dj_dw[j] + err * X[i, j]\n",
    "        dj_db = dj_db + err\n",
    "    dj_dw = dj_dw / m\n",
    "    dj_db = dj_db / m\n",
    "\n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3020/1291870043.py:21: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  dj_dw[j] = dj_dw[j] + err * X[i, j]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dj_db at initial w,b: [1400691.42680684]\n",
      "dj_dw at initial w,b: \n",
      " [7.60805508e+07 4.21737069e+06 2.27250541e+10 1.38314348e+06\n",
      " 2.52536388e+06 3.83543347e+06 1.40751205e+03 4.05231556e+06\n",
      " 1.83149409e+05 1.75256292e+07 2.84885560e+06 2.80955078e+06\n",
      " 5.17546464e+05 4.22158230e+06 8.69788988e+06 7.79058811e+06\n",
      " 2.76195913e+09 2.78090970e+09 2.00342838e+06 1.94091865e+05\n",
      " 1.34922674e+07 1.44303857e+07 2.44135874e+06 1.62856842e+08\n",
      " 3.52192038e+06 5.23249515e+06 1.95725437e+06 3.12666947e+06\n",
      " 3.94924453e+06 3.09718669e+06 3.77213241e+06 6.87910972e+08\n",
      " 6.53939371e+06 7.93722974e+07 7.94589520e+08 1.56187279e+09\n",
      " 1.45181129e+06 2.16532742e+06 1.32041871e+06 5.19587153e+06\n",
      " 5.09772053e+08 9.78783838e+06 2.22492155e+09 6.53839550e+05\n",
      " 9.58301026e+04 2.25527455e+06 5.46423320e+05 4.09058274e+06\n",
      " 1.46039231e+06 3.22620767e+06 8.03476162e+06 1.00312161e+06\n",
      " 2.88967637e+06 1.63054985e+06 2.57422874e+06 5.44573756e+06\n",
      " 5.49531588e+06 2.63111351e+06 1.52116993e+08 6.83682224e+07\n",
      " 3.02927389e+07 5.43579642e+06 2.35608171e+07 3.51649976e+06\n",
      " 7.77950001e+07 8.85487516e+06 2.81229954e+09 1.05047041e+07\n",
      " 5.29183531e+06]\n"
     ]
    }
   ],
   "source": [
    "#Compute and display gradient\n",
    "tmp_dj_db, tmp_dj_dw = compute_gradient(X_train, y_train, w_init, b_init)\n",
    "print(f'dj_db at initial w,b: {tmp_dj_db}')\n",
    "print(f'dj_dw at initial w,b: \\n {tmp_dj_dw}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters):\n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn theta. Updates theta by taking\n",
    "    num_iters gradient steps with learning rate alpha\n",
    "\n",
    "    Args:\n",
    "      X (ndarray (m,n))   : Data, m examples with n features\n",
    "      y (ndarray (m,))    : target values\n",
    "      w_in (ndarray (n,)) : initial model parameters\n",
    "      b_in (scalar)       : initial model parameter\n",
    "      cost_function       : function to compute cost\n",
    "      gradient_function   : function to compute the gradient\n",
    "      alpha (float)       : Learning rate\n",
    "      num_iters (int)     : number of iterations to run gradient descent\n",
    "\n",
    "    Returns:\n",
    "      w (ndarray (n,)) : Updated values of parameters\n",
    "      b (scalar)       : Updated value of parameter\n",
    "      \"\"\"\n",
    "\n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n",
    "    b = b_in\n",
    "\n",
    "    for i in range(num_iters):\n",
    "\n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_db,dj_dw = gradient_function(X, y, w, b)   ##None\n",
    "\n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        w = w - alpha * dj_dw               ##None\n",
    "        b = b - alpha * dj_db               ##None\n",
    "\n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion\n",
    "            J_history.append( cost_function(X, y, w, b))\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters /10) == 0:\n",
    "          pass\n",
    "            #print(f\"Iteration {i:4d}: Cost {J_history[-1]:8.2f}   \")\n",
    "\n",
    "    return w, b,J_history #return final w,b and J history for graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3020/1291870043.py:21: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  dj_dw[j] = dj_dw[j] + err * X[i, j]\n",
      "/tmp/ipykernel_3020/2373855227.py:17: RuntimeWarning: overflow encountered in add\n",
      "  cost = cost + (f_wb_i - y[i])**2       #scalar\n",
      "/tmp/ipykernel_3020/2373855227.py:17: RuntimeWarning: overflow encountered in square\n",
      "  cost = cost + (f_wb_i - y[i])**2       #scalar\n",
      "/tmp/ipykernel_3020/1291870043.py:21: RuntimeWarning: overflow encountered in add\n",
      "  dj_dw[j] = dj_dw[j] + err * X[i, j]\n",
      "/tmp/ipykernel_3020/1291870043.py:21: RuntimeWarning: invalid value encountered in multiply\n",
      "  dj_dw[j] = dj_dw[j] + err * X[i, j]\n",
      "/tmp/ipykernel_3020/3045136546.py:32: RuntimeWarning: invalid value encountered in subtract\n",
      "  w = w - alpha * dj_dw               ##None\n"
     ]
    }
   ],
   "source": [
    "# initialize parameters\n",
    "initial_w = np.zeros_like(w_init)\n",
    "initial_b = 0.\n",
    "# some gradient descent settings\n",
    "iterations = 1000\n",
    "alpha = 5.0e-7\n",
    "# run gradient descent\n",
    "w_final, b_final, J_hist = gradient_descent(X_train, y_train, initial_w, initial_b,\n",
    "                                                    compute_cost, compute_gradient,\n",
    "                                                    alpha, iterations)\n",
    "print(f\"b,w found by gradient descent: {b_final:0.2f},{w_final} \")\n",
    "m,_ = X_train.shape\n",
    "for i in range(m):\n",
    "    print(f\"prediction: {np.dot(X_train[i], w_final) + b_final:0.2f}, target value: {y_train[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot cost versus iteration\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, constrained_layout=True, figsize=(12, 4))\n",
    "ax1.plot(J_hist)\n",
    "ax2.plot(100 + np.arange(len(J_hist[100:])), J_hist[100:])\n",
    "ax1.set_title(\"Cost vs. iteration\");  ax2.set_title(\"Cost vs. iteration (tail)\")\n",
    "ax1.set_ylabel('Cost')             ;  ax2.set_ylabel('Cost')\n",
    "ax1.set_xlabel('iteration step')   ;  ax2.set_xlabel('iteration step')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
